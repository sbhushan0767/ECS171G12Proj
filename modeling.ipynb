{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in processed data\n",
    "df = pd.read_csv('./datasets/credit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  \n",
       "21         0.969880       0.010111                   1.0             0.0   \n",
       "22         0.915663       0.008613                   0.6             1.0   \n",
       "23         0.867470       0.007687                   0.2             1.0   \n",
       "24         0.891566       0.002707                   0.0             1.0   \n",
       "25         0.843373       0.007081                   1.0             0.0   \n",
       "26         0.963855       0.007158                   0.5             0.0   \n",
       "27         0.951807       0.004082                   0.4             0.0   \n",
       "28         0.813253       0.004350                   0.8             0.0   \n",
       "29         0.801205       0.008327                   0.8             0.0   \n",
       "...             ...            ...                   ...             ...   \n",
       "78620      0.855422       0.012095                   0.4             1.0   \n",
       "78621      0.777108       0.012179                   0.0             0.0   \n",
       "78622      0.572289       0.005359                   1.0             1.0   \n",
       "78623      0.873494       0.008514                   1.0             0.0   \n",
       "78624      0.951807       0.015240                   0.0             1.0   \n",
       "78625      0.897590       0.006029                   1.0             1.0   \n",
       "78626      0.843373       0.004647                   1.0             1.0   \n",
       "78627      0.512048       0.014205                   0.5             0.0   \n",
       "78628      0.837349       0.004603                   1.0             1.0   \n",
       "78629      0.969880       0.003172                   0.6             1.0   \n",
       "78630      0.903614       0.008350                   0.5             1.0   \n",
       "78631      0.475904       0.009817                   1.0             0.0   \n",
       "78632      0.987952       0.013377                   0.6             1.0   \n",
       "78633      0.746988       0.006502                   1.0             0.0   \n",
       "78634      0.608434       0.005758                   0.0             1.0   \n",
       "78635      1.000000       0.006117                   0.8             0.0   \n",
       "78636      0.789157       0.009158                   1.0             0.5   \n",
       "78637      0.933735       0.005173                   0.2             1.0   \n",
       "78638      0.801205       0.007600                   0.0             0.0   \n",
       "78639      0.939759       0.004814                   0.9             1.0   \n",
       "78640      0.909639       0.003359                   0.2             1.0   \n",
       "78641      0.156627       0.018769                   0.4             0.0   \n",
       "78642      0.608434       0.005059                   0.3             1.0   \n",
       "78643      0.674699       0.012729                   0.6             0.0   \n",
       "78644      0.915663       0.006995                   0.0             1.0   \n",
       "78645      0.927711       0.008776                   0.7             0.0   \n",
       "78646      0.879518       0.008688                   0.6             1.0   \n",
       "78647      0.801205       0.004149                   0.4             0.0   \n",
       "78648      0.819277       0.005411                   1.0             0.0   \n",
       "78649      0.981928       0.006063                   0.6             0.0   \n",
       "\n",
       "        Purpose  Monthly Debt  Years of Credit History  \\\n",
       "0      0.416667      0.011965                 0.202096   \n",
       "1      0.250000      0.066998                 0.167665   \n",
       "2      0.250000      0.020057                 0.124251   \n",
       "3      0.250000      0.037554                 0.203593   \n",
       "4      0.250000      0.024906                 0.238024   \n",
       "5      0.250000      0.042814                 0.282934   \n",
       "6      0.250000      0.090119                 0.152695   \n",
       "7      0.250000      0.027191                 0.184132   \n",
       "8      0.250000      0.026565                 0.071856   \n",
       "9      0.250000      0.040291                 0.143713   \n",
       "10     0.250000      0.032606                 0.314371   \n",
       "11     0.250000      0.040410                 0.273952   \n",
       "12     0.416667      0.005687                 0.284431   \n",
       "13     0.250000      0.022102                 0.205090   \n",
       "14     0.250000      0.030291                 0.122754   \n",
       "15     0.250000      0.050248                 0.179641   \n",
       "16     0.250000      0.013447                 0.206587   \n",
       "17     0.250000      0.015630                 0.160180   \n",
       "18     0.083333      0.035902                 0.278443   \n",
       "19     0.250000      0.031702                 0.124251   \n",
       "20     0.250000      0.057787                 0.154192   \n",
       "21     0.250000      0.044161                 0.244012   \n",
       "22     0.000000      0.071217                 0.214072   \n",
       "23     0.250000      0.038808                 0.244012   \n",
       "24     0.250000      0.021365                 0.175150   \n",
       "25     0.250000      0.041770                 0.163174   \n",
       "26     0.250000      0.047259                 0.311377   \n",
       "27     0.250000      0.039111                 0.155689   \n",
       "28     0.250000      0.007812                 0.282934   \n",
       "29     0.250000      0.030035                 0.375749   \n",
       "...         ...           ...                      ...   \n",
       "78620  0.250000      0.022052                 0.154192   \n",
       "78621  0.250000      0.035041                 0.181138   \n",
       "78622  0.583333      0.030396                 0.206587   \n",
       "78623  0.500000      0.028972                 0.423653   \n",
       "78624  0.583333      0.078503                 0.263473   \n",
       "78625  0.250000      0.037590                 0.257485   \n",
       "78626  0.250000      0.064031                 0.169162   \n",
       "78627  0.250000      0.086785                 0.203593   \n",
       "78628  0.250000      0.045682                 0.300898   \n",
       "78629  0.166667      0.015445                 0.384731   \n",
       "78630  0.583333      0.043223                 0.215569   \n",
       "78631  0.250000      0.073832                 0.173653   \n",
       "78632  0.250000      0.080573                 0.176647   \n",
       "78633  0.250000      0.037907                 0.261976   \n",
       "78634  0.583333      0.043700                 0.200599   \n",
       "78635  0.250000      0.025192                 0.350299   \n",
       "78636  0.250000      0.030745                 0.215569   \n",
       "78637  0.250000      0.042622                 0.067365   \n",
       "78638  0.250000      0.060716                 0.419162   \n",
       "78639  0.250000      0.025881                 0.258982   \n",
       "78640  0.250000      0.030108                 0.155689   \n",
       "78641  0.250000      0.182550                 0.254491   \n",
       "78642  0.250000      0.022015                 0.079341   \n",
       "78643  0.250000      0.038734                 0.275449   \n",
       "78644  0.250000      0.046720                 0.086826   \n",
       "78645  0.250000      0.044728                 0.194611   \n",
       "78646  0.250000      0.011003                 0.131737   \n",
       "78647  0.250000      0.028455                 0.092814   \n",
       "78648  0.250000      0.028066                 0.196108   \n",
       "78649  0.250000      0.028083                 0.239521   \n",
       "\n",
       "       Number of Open Accounts  Have had Credit Problems  \\\n",
       "0                     0.066667                       1.0   \n",
       "1                     0.226667                       1.0   \n",
       "2                     0.106667                       0.0   \n",
       "3                     0.066667                       0.0   \n",
       "4                     0.160000                       1.0   \n",
       "5                     0.040000                       0.0   \n",
       "6                     0.253333                       0.0   \n",
       "7                     0.200000                       0.0   \n",
       "8                     0.066667                       0.0   \n",
       "9                     0.120000                       1.0   \n",
       "10                    0.120000                       1.0   \n",
       "11                    0.186667                       0.0   \n",
       "12                    0.066667                       0.0   \n",
       "13                    0.040000                       0.0   \n",
       "14                    0.080000                       0.0   \n",
       "15                    0.146667                       0.0   \n",
       "16                    0.080000                       0.0   \n",
       "17                    0.066667                       1.0   \n",
       "18                    0.080000                       0.0   \n",
       "19                    0.066667                       0.0   \n",
       "20                    0.186667                       0.0   \n",
       "21                    0.213333                       0.0   \n",
       "22                    0.093333                       0.0   \n",
       "23                    0.200000                       0.0   \n",
       "24                    0.080000                       1.0   \n",
       "25                    0.226667                       0.0   \n",
       "26                    0.160000                       0.0   \n",
       "27                    0.120000                       0.0   \n",
       "28                    0.066667                       1.0   \n",
       "29                    0.173333                       0.0   \n",
       "...                        ...                       ...   \n",
       "78620                 0.080000                       0.0   \n",
       "78621                 0.226667                       1.0   \n",
       "78622                 0.173333                       1.0   \n",
       "78623                 0.080000                       0.0   \n",
       "78624                 0.133333                       0.0   \n",
       "78625                 0.146667                       0.0   \n",
       "78626                 0.146667                       1.0   \n",
       "78627                 0.186667                       1.0   \n",
       "78628                 0.106667                       0.0   \n",
       "78629                 0.080000                       0.0   \n",
       "78630                 0.080000                       0.0   \n",
       "78631                 0.186667                       0.0   \n",
       "78632                 0.106667                       0.0   \n",
       "78633                 0.106667                       0.0   \n",
       "78634                 0.146667                       0.0   \n",
       "78635                 0.146667                       0.0   \n",
       "78636                 0.186667                       0.0   \n",
       "78637                 0.093333                       0.0   \n",
       "78638                 0.306667                       0.0   \n",
       "78639                 0.080000                       0.0   \n",
       "78640                 0.213333                       0.0   \n",
       "78641                 0.613333                       0.0   \n",
       "78642                 0.053333                       0.0   \n",
       "78643                 0.120000                       0.0   \n",
       "78644                 0.120000                       0.0   \n",
       "78645                 0.133333                       0.0   \n",
       "78646                 0.106667                       0.0   \n",
       "78647                 0.093333                       0.0   \n",
       "78648                 0.093333                       1.0   \n",
       "78649                 0.173333                       0.0   \n",
       "\n",
       "       Current Credit Balance  Maximum Open Credit  \\\n",
       "0                    0.006940             0.000271   \n",
       "1                    0.009063             0.000487   \n",
       "2                    0.007796             0.000251   \n",
       "3                    0.006549             0.000177   \n",
       "4                    0.003716             0.000177   \n",
       "5                    0.013296             0.000360   \n",
       "6                    0.020364             0.000663   \n",
       "7                    0.006204             0.000188   \n",
       "8                    0.004078             0.000143   \n",
       "9                    0.006860             0.000322   \n",
       "10                   0.000860             0.000070   \n",
       "11                   0.024748             0.001302   \n",
       "12                   0.003686             0.000521   \n",
       "13                   0.001834             0.000082   \n",
       "14                   0.004013             0.000298   \n",
       "15                   0.027117             0.000702   \n",
       "16                   0.002908             0.000150   \n",
       "17                   0.004351             0.000159   \n",
       "18                   0.003271             0.000317   \n",
       "19                   0.004208             0.000144   \n",
       "20                   0.010416             0.000588   \n",
       "21                   0.006825             0.000192   \n",
       "22                   0.006976             0.000305   \n",
       "23                   0.009525             0.000350   \n",
       "24                   0.003975             0.000175   \n",
       "25                   0.009154             0.000335   \n",
       "26                   0.020828             0.000648   \n",
       "27                   0.006493             0.000311   \n",
       "28                   0.003470             0.000110   \n",
       "29                   0.005900             0.000298   \n",
       "...                       ...                  ...   \n",
       "78620                0.003426             0.000390   \n",
       "78621                0.003704             0.000211   \n",
       "78622                0.002733             0.000161   \n",
       "78623                0.006895             0.000189   \n",
       "78624                0.007767             0.000000   \n",
       "78625                0.005923             0.000190   \n",
       "78626                0.009004             0.000305   \n",
       "78627                0.010807             0.000305   \n",
       "78628                0.006748             0.000190   \n",
       "78629                0.000346             0.000175   \n",
       "78630                0.004362             0.000140   \n",
       "78631                0.016656             0.000454   \n",
       "78632                0.009795             0.000373   \n",
       "78633                0.007822             0.000224   \n",
       "78634                0.001566             0.000269   \n",
       "78635                0.008590             0.000492   \n",
       "78636                0.005010             0.000197   \n",
       "78637                0.001116             0.000161   \n",
       "78638                0.024000             0.001820   \n",
       "78639                0.010953             0.000300   \n",
       "78640                0.006538             0.000211   \n",
       "78641                0.042548             0.002385   \n",
       "78642                0.009112             0.000225   \n",
       "78643                0.011715             0.000341   \n",
       "78644                0.008314             0.000266   \n",
       "78645                0.012751             0.000427   \n",
       "78646                0.002648             0.000152   \n",
       "78647                0.002260             0.000214   \n",
       "78648                0.005626             0.000156   \n",
       "78649                0.005445             0.000395   \n",
       "\n",
       "       Have had Bankruptcy before  Have had Tax Liens  Delinquent Time  \\\n",
       "0                             1.0                 0.0         0.000000   \n",
       "1                             0.0                 0.0         0.666667   \n",
       "2                             0.0                 0.0         0.000000   \n",
       "3                             0.0                 0.0         0.000000   \n",
       "4                             1.0                 0.0         0.333333   \n",
       "5                             0.0                 0.0         0.666667   \n",
       "6                             0.0                 0.0         0.000000   \n",
       "7                             0.0                 0.0         1.000000   \n",
       "8                             0.0                 0.0         0.333333   \n",
       "9                             1.0                 0.0         0.000000   \n",
       "10                            1.0                 0.0         0.666667   \n",
       "11                            0.0                 0.0         0.666667   \n",
       "12                            0.0                 0.0         0.000000   \n",
       "13                            0.0                 0.0         1.000000   \n",
       "14                            0.0                 0.0         0.000000   \n",
       "15                            0.0                 0.0         0.000000   \n",
       "16                            0.0                 0.0         0.000000   \n",
       "17                            0.0                 1.0         0.000000   \n",
       "18                            0.0                 0.0         0.666667   \n",
       "19                            0.0                 0.0         0.000000   \n",
       "20                            0.0                 0.0         0.000000   \n",
       "21                            0.0                 0.0         0.666667   \n",
       "22                            0.0                 0.0         0.000000   \n",
       "23                            0.0                 0.0         0.000000   \n",
       "24                            1.0                 0.0         0.000000   \n",
       "25                            0.0                 0.0         0.000000   \n",
       "26                            0.0                 0.0         0.000000   \n",
       "27                            0.0                 0.0         0.000000   \n",
       "28                            0.0                 0.0         0.000000   \n",
       "29                            0.0                 0.0         0.333333   \n",
       "...                           ...                 ...              ...   \n",
       "78620                         0.0                 0.0         0.666667   \n",
       "78621                         1.0                 0.0         1.000000   \n",
       "78622                         1.0                 0.0         0.000000   \n",
       "78623                         0.0                 0.0         0.000000   \n",
       "78624                         0.0                 0.0         0.666667   \n",
       "78625                         0.0                 0.0         1.000000   \n",
       "78626                         1.0                 0.0         0.000000   \n",
       "78627                         1.0                 0.0         0.000000   \n",
       "78628                         0.0                 0.0         1.000000   \n",
       "78629                         0.0                 0.0         0.000000   \n",
       "78630                         0.0                 0.0         0.333333   \n",
       "78631                         0.0                 0.0         0.000000   \n",
       "78632                         0.0                 0.0         0.000000   \n",
       "78633                         0.0                 0.0         1.000000   \n",
       "78634                         0.0                 0.0         0.333333   \n",
       "78635                         0.0                 0.0         0.666667   \n",
       "78636                         0.0                 0.0         0.666667   \n",
       "78637                         0.0                 0.0         0.000000   \n",
       "78638                         0.0                 0.0         1.000000   \n",
       "78639                         0.0                 0.0         0.666667   \n",
       "78640                         0.0                 0.0         0.666667   \n",
       "78641                         0.0                 0.0         0.666667   \n",
       "78642                         0.0                 0.0         0.000000   \n",
       "78643                         0.0                 0.0         0.000000   \n",
       "78644                         0.0                 0.0         0.000000   \n",
       "78645                         0.0                 0.0         1.000000   \n",
       "78646                         0.0                 0.0         0.000000   \n",
       "78647                         0.0                 0.0         0.333333   \n",
       "78648                         0.0                 0.0         0.333333   \n",
       "78649                         0.0                 0.0         0.000000   \n",
       "\n",
       "      Credit Score Range  \n",
       "0         (701.2, 717.8]  \n",
       "1         (734.4, 751.0]  \n",
       "2         (717.8, 734.4]  \n",
       "3         (717.8, 734.4]  \n",
       "4         (717.8, 734.4]  \n",
       "5         (668.0, 684.6]  \n",
       "6         (734.4, 751.0]  \n",
       "7         (717.8, 734.4]  \n",
       "8         (734.4, 751.0]  \n",
       "9         (734.4, 751.0]  \n",
       "10        (717.8, 734.4]  \n",
       "11        (717.8, 734.4]  \n",
       "12        (734.4, 751.0]  \n",
       "13        (684.6, 701.2]  \n",
       "14        (734.4, 751.0]  \n",
       "15        (701.2, 717.8]  \n",
       "16        (717.8, 734.4]  \n",
       "17        (701.2, 717.8]  \n",
       "18        (684.6, 701.2]  \n",
       "19        (717.8, 734.4]  \n",
       "20        (734.4, 751.0]  \n",
       "21        (734.4, 751.0]  \n",
       "22        (734.4, 751.0]  \n",
       "23        (717.8, 734.4]  \n",
       "24        (717.8, 734.4]  \n",
       "25        (717.8, 734.4]  \n",
       "26        (734.4, 751.0]  \n",
       "27        (734.4, 751.0]  \n",
       "28        (717.8, 734.4]  \n",
       "29        (717.8, 734.4]  \n",
       "...                  ...  \n",
       "78620     (717.8, 734.4]  \n",
       "78621     (701.2, 717.8]  \n",
       "78622     (668.0, 684.6]  \n",
       "78623     (717.8, 734.4]  \n",
       "78624     (734.4, 751.0]  \n",
       "78625     (717.8, 734.4]  \n",
       "78626     (717.8, 734.4]  \n",
       "78627     (668.0, 684.6]  \n",
       "78628     (717.8, 734.4]  \n",
       "78629     (734.4, 751.0]  \n",
       "78630     (734.4, 751.0]  \n",
       "78631     (651.4, 668.0]  \n",
       "78632     (734.4, 751.0]  \n",
       "78633     (701.2, 717.8]  \n",
       "78634     (684.6, 701.2]  \n",
       "78635     (734.4, 751.0]  \n",
       "78636     (701.2, 717.8]  \n",
       "78637     (734.4, 751.0]  \n",
       "78638     (717.8, 734.4]  \n",
       "78639     (734.4, 751.0]  \n",
       "78640     (734.4, 751.0]  \n",
       "78641     (601.6, 618.2]  \n",
       "78642     (684.6, 701.2]  \n",
       "78643     (684.6, 701.2]  \n",
       "78644     (734.4, 751.0]  \n",
       "78645     (734.4, 751.0]  \n",
       "78646     (717.8, 734.4]  \n",
       "78647     (717.8, 734.4]  \n",
       "78648     (717.8, 734.4]  \n",
       "78649     (734.4, 751.0]  \n",
       "\n",
       "[78650 rows x 20 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Loan ID</th>\n      <th>Customer ID</th>\n      <th>Loan Status</th>\n      <th>Current Loan Amount</th>\n      <th>Term</th>\n      <th>Credit Score</th>\n      <th>Annual Income</th>\n      <th>Years in current job</th>\n      <th>Home Ownership</th>\n      <th>Purpose</th>\n      <th>Monthly Debt</th>\n      <th>Years of Credit History</th>\n      <th>Number of Open Accounts</th>\n      <th>Have had Credit Problems</th>\n      <th>Current Credit Balance</th>\n      <th>Maximum Open Credit</th>\n      <th>Have had Bankruptcy before</th>\n      <th>Have had Tax Liens</th>\n      <th>Delinquent Time</th>\n      <th>Credit Score Range</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.081528</td>\n      <td>0.594985</td>\n      <td>1.0</td>\n      <td>0.004342</td>\n      <td>1.0</td>\n      <td>0.746988</td>\n      <td>0.006592</td>\n      <td>0.8</td>\n      <td>0.0</td>\n      <td>0.416667</td>\n      <td>0.011965</td>\n      <td>0.202096</td>\n      <td>0.066667</td>\n      <td>1.0</td>\n      <td>0.006940</td>\n      <td>0.000271</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>(701.2, 717.8]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.309318</td>\n      <td>0.371466</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n      <td>0.939759</td>\n      <td>0.013024</td>\n      <td>0.8</td>\n      <td>0.5</td>\n      <td>0.250000</td>\n      <td>0.066998</td>\n      <td>0.167665</td>\n      <td>0.226667</td>\n      <td>1.0</td>\n      <td>0.009063</td>\n      <td>0.000487</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.666667</td>\n      <td>(734.4, 751.0]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.467122</td>\n      <td>0.903985</td>\n      <td>1.0</td>\n      <td>0.003365</td>\n      <td>0.0</td>\n      <td>0.819277</td>\n      <td>0.004413</td>\n      <td>0.3</td>\n      <td>0.5</td>\n      <td>0.250000</td>\n      <td>0.020057</td>\n      <td>0.124251</td>\n      <td>0.106667</td>\n      <td>0.0</td>\n      <td>0.007796</td>\n      <td>0.000251</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>(717.8, 734.4]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.539748</td>\n      <td>0.312567</td>\n      <td>0.0</td>\n      <td>0.001954</td>\n      <td>1.0</td>\n      <td>0.867470</td>\n      <td>0.004957</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.250000</td>\n      <td>0.037554</td>\n      <td>0.203593</td>\n      <td>0.066667</td>\n      <td>0.0</td>\n      <td>0.006549</td>\n      <td>0.000177</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>(717.8, 734.4]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.153657</td>\n      <td>0.565583</td>\n      <td>1.0</td>\n      <td>0.002064</td>\n      <td>1.0</td>\n      <td>0.873494</td>\n      <td>0.006693</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.250000</td>\n      <td>0.024906</td>\n      <td>0.238024</td>\n      <td>0.160000</td>\n      <td>1.0</td>\n      <td>0.003716</td>\n      <td>0.000177</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.333333</td>\n      <td>(717.8, 734.4]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.544144</td>\n      <td>0.686563</td>\n      <td>1.0</td>\n      <td>0.005376</td>\n      <td>1.0</td>\n      <td>0.560241</td>\n      <td>0.015002</td>\n      <td>0.2</td>\n      <td>1.0</td>\n      <td>0.250000</td>\n      <td>0.042814</td>\n      <td>0.282934</td>\n      <td>0.040000</td>\n      <td>0.0</td>\n      <td>0.013296</td>\n      <td>0.000360</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.666667</td>\n      <td>(668.0, 684.6]</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.042962</td>\n      <td>0.138742</td>\n      <td>1.0</td>\n      <td>0.002047</td>\n      <td>1.0</td>\n      <td>0.927711</td>\n      <td>0.008328</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.250000</td>\n      <td>0.090119</td>\n      <td>0.152695</td>\n      <td>0.253333</td>\n      <td>0.0</td>\n      <td>0.020364</td>\n      <td>0.000663</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>(734.4, 751.0]</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.198139</td>\n      <td>0.053855</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n      <td>0.861446</td>\n      <td>0.003855</td>\n      <td>0.3</td>\n      <td>1.0</td>\n      <td>0.250000</td>\n      <td>0.027191</td>\n      <td>0.184132</td>\n      <td>0.200000</td>\n      <td>0.0</td>\n      <td>0.006204</td>\n      <td>0.000188</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.000000</td>\n      <td>(717.8, 734.4]</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.251344</td>\n      <td>0.067429</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n      <td>0.933735</td>\n      <td>0.004227</td>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>0.250000</td>\n      <td>0.026565</td>\n      <td>0.071856</td>\n      <td>0.066667</td>\n      <td>0.0</td>\n      <td>0.004078</td>\n      <td>0.000143</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.333333</td>\n      <td>(734.4, 751.0]</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.006981</td>\n      <td>0.562819</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n      <td>0.951807</td>\n      <td>0.008970</td>\n      <td>0.4</td>\n      <td>1.0</td>\n      <td>0.250000</td>\n      <td>0.040291</td>\n      <td>0.143713</td>\n      <td>0.120000</td>\n      <td>1.0</td>\n      <td>0.006860</td>\n      <td>0.000322</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>(734.4, 751.0]</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.182077</td>\n      <td>0.167038</td>\n      <td>1.0</td>\n      <td>0.002229</td>\n      <td>1.0</td>\n      <td>0.855422</td>\n      <td>0.003726</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.250000</td>\n      <td>0.032606</td>\n      <td>0.314371</td>\n      <td>0.120000</td>\n      <td>1.0</td>\n      <td>0.000860</td>\n      <td>0.000070</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.666667</td>\n      <td>(717.8, 734.4]</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.143953</td>\n      <td>0.588364</td>\n      <td>1.0</td>\n      <td>0.006550</td>\n      <td>0.0</td>\n      <td>0.831325</td>\n      <td>0.010547</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.250000</td>\n      <td>0.040410</td>\n      <td>0.273952</td>\n      <td>0.186667</td>\n      <td>0.0</td>\n      <td>0.024748</td>\n      <td>0.001302</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.666667</td>\n      <td>(717.8, 734.4]</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.257855</td>\n      <td>0.258560</td>\n      <td>1.0</td>\n      <td>0.003792</td>\n      <td>1.0</td>\n      <td>0.975904</td>\n      <td>0.010364</td>\n      <td>0.8</td>\n      <td>0.0</td>\n      <td>0.416667</td>\n      <td>0.005687</td>\n      <td>0.284431</td>\n      <td>0.066667</td>\n      <td>0.0</td>\n      <td>0.003686</td>\n      <td>0.000521</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>(734.4, 751.0]</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.082454</td>\n      <td>0.253653</td>\n      <td>0.0</td>\n      <td>0.003059</td>\n      <td>0.0</td>\n      <td>0.614458</td>\n      <td>0.006385</td>\n      <td>0.8</td>\n      <td>1.0</td>\n      <td>0.250000</td>\n      <td>0.022102</td>\n      <td>0.205090</td>\n      <td>0.040000</td>\n      <td>0.0</td>\n      <td>0.001834</td>\n      <td>0.000082</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.000000</td>\n      <td>(684.6, 701.2]</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.194200</td>\n      <td>0.005667</td>\n      <td>1.0</td>\n      <td>0.001170</td>\n      <td>1.0</td>\n      <td>0.993976</td>\n      <td>0.007720</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.250000</td>\n      <td>0.030291</td>\n      <td>0.122754</td>\n      <td>0.080000</td>\n      <td>0.0</td>\n      <td>0.004013</td>\n      <td>0.000298</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>(734.4, 751.0]</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.781720</td>\n      <td>0.356399</td>\n      <td>0.0</td>\n      <td>0.001420</td>\n      <td>1.0</td>\n      <td>0.777108</td>\n      <td>0.010962</td>\n      <td>0.2</td>\n      <td>1.0</td>\n      <td>0.250000</td>\n      <td>0.050248</td>\n      <td>0.179641</td>\n      <td>0.146667</td>\n      <td>0.0</td>\n      <td>0.027117</td>\n      <td>0.000702</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>(701.2, 717.8]</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.081183</td>\n      <td>0.917974</td>\n      <td>1.0</td>\n      <td>0.000807</td>\n      <td>1.0</td>\n      <td>0.837349</td>\n      <td>0.004676</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.250000</td>\n      <td>0.013447</td>\n      <td>0.206587</td>\n      <td>0.080000</td>\n      <td>0.0</td>\n      <td>0.002908</td>\n      <td>0.000150</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>(717.8, 734.4]</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.480323</td>\n      <td>0.331587</td>\n      <td>1.0</td>\n      <td>0.002337</td>\n      <td>0.0</td>\n      <td>0.716867</td>\n      <td>0.007090</td>\n      <td>0.4</td>\n      <td>0.0</td>\n      <td>0.250000</td>\n      <td>0.015630</td>\n      <td>0.160180</td>\n      <td>0.066667</td>\n      <td>1.0</td>\n      <td>0.004351</td>\n      <td>0.000159</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.000000</td>\n      <td>(701.2, 717.8]</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.342701</td>\n      <td>0.797119</td>\n      <td>1.0</td>\n      <td>0.004542</td>\n      <td>0.0</td>\n      <td>0.620482</td>\n      <td>0.009947</td>\n      <td>0.3</td>\n      <td>1.0</td>\n      <td>0.083333</td>\n      <td>0.035902</td>\n      <td>0.278443</td>\n      <td>0.080000</td>\n      <td>0.0</td>\n      <td>0.003271</td>\n      <td>0.000317</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.666667</td>\n      <td>(684.6, 701.2]</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.047137</td>\n      <td>0.609057</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n      <td>0.837349</td>\n      <td>0.005760</td>\n      <td>0.1</td>\n      <td>1.0</td>\n      <td>0.250000</td>\n      <td>0.031702</td>\n      <td>0.124251</td>\n      <td>0.066667</td>\n      <td>0.0</td>\n      <td>0.004208</td>\n      <td>0.000144</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>(717.8, 734.4]</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.096360</td>\n      <td>0.170963</td>\n      <td>1.0</td>\n      <td>0.004328</td>\n      <td>1.0</td>\n      <td>0.987952</td>\n      <td>0.008193</td>\n      <td>0.2</td>\n      <td>0.0</td>\n      <td>0.250000</td>\n      <td>0.057787</td>\n      <td>0.154192</td>\n      <td>0.186667</td>\n      <td>0.0</td>\n      <td>0.010416</td>\n      <td>0.000588</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>(734.4, 751.0]</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.800741</td>\n      <td>0.410116</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n      <td>0.969880</td>\n      <td>0.010111</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.250000</td>\n      <td>0.044161</td>\n      <td>0.244012</td>\n      <td>0.213333</td>\n      <td>0.0</td>\n      <td>0.006825</td>\n      <td>0.000192</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.666667</td>\n      <td>(734.4, 751.0]</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.717664</td>\n      <td>0.952974</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n      <td>0.915663</td>\n      <td>0.008613</td>\n      <td>0.6</td>\n      <td>1.0</td>\n      <td>0.000000</td>\n      <td>0.071217</td>\n      <td>0.214072</td>\n      <td>0.093333</td>\n      <td>0.0</td>\n      <td>0.006976</td>\n      <td>0.000305</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>(734.4, 751.0]</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.600473</td>\n      <td>0.143055</td>\n      <td>1.0</td>\n      <td>0.003234</td>\n      <td>1.0</td>\n      <td>0.867470</td>\n      <td>0.007687</td>\n      <td>0.2</td>\n      <td>1.0</td>\n      <td>0.250000</td>\n      <td>0.038808</td>\n      <td>0.244012</td>\n      <td>0.200000</td>\n      <td>0.0</td>\n      <td>0.009525</td>\n      <td>0.000350</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>(717.8, 734.4]</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.995991</td>\n      <td>0.112409</td>\n      <td>0.0</td>\n      <td>0.001189</td>\n      <td>1.0</td>\n      <td>0.891566</td>\n      <td>0.002707</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.250000</td>\n      <td>0.021365</td>\n      <td>0.175150</td>\n      <td>0.080000</td>\n      <td>1.0</td>\n      <td>0.003975</td>\n      <td>0.000175</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>(717.8, 734.4]</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.686134</td>\n      <td>0.381903</td>\n      <td>1.0</td>\n      <td>0.003224</td>\n      <td>0.0</td>\n      <td>0.843373</td>\n      <td>0.007081</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.250000</td>\n      <td>0.041770</td>\n      <td>0.163174</td>\n      <td>0.226667</td>\n      <td>0.0</td>\n      <td>0.009154</td>\n      <td>0.000335</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>(717.8, 734.4]</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.405278</td>\n      <td>0.724396</td>\n      <td>1.0</td>\n      <td>0.001146</td>\n      <td>1.0</td>\n      <td>0.963855</td>\n      <td>0.007158</td>\n      <td>0.5</td>\n      <td>0.0</td>\n      <td>0.250000</td>\n      <td>0.047259</td>\n      <td>0.311377</td>\n      <td>0.160000</td>\n      <td>0.0</td>\n      <td>0.020828</td>\n      <td>0.000648</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>(734.4, 751.0]</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.004271</td>\n      <td>0.526202</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n      <td>0.951807</td>\n      <td>0.004082</td>\n      <td>0.4</td>\n      <td>0.0</td>\n      <td>0.250000</td>\n      <td>0.039111</td>\n      <td>0.155689</td>\n      <td>0.120000</td>\n      <td>0.0</td>\n      <td>0.006493</td>\n      <td>0.000311</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>(734.4, 751.0]</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.839266</td>\n      <td>0.820646</td>\n      <td>1.0</td>\n      <td>0.001499</td>\n      <td>1.0</td>\n      <td>0.813253</td>\n      <td>0.004350</td>\n      <td>0.8</td>\n      <td>0.0</td>\n      <td>0.250000</td>\n      <td>0.007812</td>\n      <td>0.282934</td>\n      <td>0.066667</td>\n      <td>1.0</td>\n      <td>0.003470</td>\n      <td>0.000110</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>(717.8, 734.4]</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.969133</td>\n      <td>0.965995</td>\n      <td>1.0</td>\n      <td>0.004379</td>\n      <td>1.0</td>\n      <td>0.801205</td>\n      <td>0.008327</td>\n      <td>0.8</td>\n      <td>0.0</td>\n      <td>0.250000</td>\n      <td>0.030035</td>\n      <td>0.375749</td>\n      <td>0.173333</td>\n      <td>0.0</td>\n      <td>0.005900</td>\n      <td>0.000298</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.333333</td>\n      <td>(717.8, 734.4]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>78620</th>\n      <td>0.706979</td>\n      <td>0.581383</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n      <td>0.855422</td>\n      <td>0.012095</td>\n      <td>0.4</td>\n      <td>1.0</td>\n      <td>0.250000</td>\n      <td>0.022052</td>\n      <td>0.154192</td>\n      <td>0.080000</td>\n      <td>0.0</td>\n      <td>0.003426</td>\n      <td>0.000390</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.666667</td>\n      <td>(717.8, 734.4]</td>\n    </tr>\n    <tr>\n      <th>78621</th>\n      <td>0.294956</td>\n      <td>0.322920</td>\n      <td>1.0</td>\n      <td>0.002531</td>\n      <td>1.0</td>\n      <td>0.777108</td>\n      <td>0.012179</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.250000</td>\n      <td>0.035041</td>\n      <td>0.181138</td>\n      <td>0.226667</td>\n      <td>1.0</td>\n      <td>0.003704</td>\n      <td>0.000211</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.000000</td>\n      <td>(701.2, 717.8]</td>\n    </tr>\n    <tr>\n      <th>78622</th>\n      <td>0.515060</td>\n      <td>0.688940</td>\n      <td>1.0</td>\n      <td>0.001282</td>\n      <td>1.0</td>\n      <td>0.572289</td>\n      <td>0.005359</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.583333</td>\n      <td>0.030396</td>\n      <td>0.206587</td>\n      <td>0.173333</td>\n      <td>1.0</td>\n      <td>0.002733</td>\n      <td>0.000161</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>(668.0, 684.6]</td>\n    </tr>\n    <tr>\n      <th>78623</th>\n      <td>0.595317</td>\n      <td>0.733174</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n      <td>0.873494</td>\n      <td>0.008514</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.500000</td>\n      <td>0.028972</td>\n      <td>0.423653</td>\n      <td>0.080000</td>\n      <td>0.0</td>\n      <td>0.006895</td>\n      <td>0.000189</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>(717.8, 734.4]</td>\n    </tr>\n    <tr>\n      <th>78624</th>\n      <td>0.418368</td>\n      <td>0.653139</td>\n      <td>1.0</td>\n      <td>0.004346</td>\n      <td>1.0</td>\n      <td>0.951807</td>\n      <td>0.015240</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.583333</td>\n      <td>0.078503</td>\n      <td>0.263473</td>\n      <td>0.133333</td>\n      <td>0.0</td>\n      <td>0.007767</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.666667</td>\n      <td>(734.4, 751.0]</td>\n    </tr>\n    <tr>\n      <th>78625</th>\n      <td>0.197780</td>\n      <td>0.576227</td>\n      <td>1.0</td>\n      <td>0.002326</td>\n      <td>1.0</td>\n      <td>0.897590</td>\n      <td>0.006029</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.250000</td>\n      <td>0.037590</td>\n      <td>0.257485</td>\n      <td>0.146667</td>\n      <td>0.0</td>\n      <td>0.005923</td>\n      <td>0.000190</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.000000</td>\n      <td>(717.8, 734.4]</td>\n    </tr>\n    <tr>\n      <th>78626</th>\n      <td>0.831622</td>\n      <td>0.143760</td>\n      <td>1.0</td>\n      <td>0.002499</td>\n      <td>1.0</td>\n      <td>0.843373</td>\n      <td>0.004647</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.250000</td>\n      <td>0.064031</td>\n      <td>0.169162</td>\n      <td>0.146667</td>\n      <td>1.0</td>\n      <td>0.009004</td>\n      <td>0.000305</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>(717.8, 734.4]</td>\n    </tr>\n    <tr>\n      <th>78627</th>\n      <td>0.572675</td>\n      <td>0.941750</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>0.0</td>\n      <td>0.512048</td>\n      <td>0.014205</td>\n      <td>0.5</td>\n      <td>0.0</td>\n      <td>0.250000</td>\n      <td>0.086785</td>\n      <td>0.203593</td>\n      <td>0.186667</td>\n      <td>1.0</td>\n      <td>0.010807</td>\n      <td>0.000305</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>(668.0, 684.6]</td>\n    </tr>\n    <tr>\n      <th>78628</th>\n      <td>0.322326</td>\n      <td>0.082759</td>\n      <td>1.0</td>\n      <td>0.003124</td>\n      <td>1.0</td>\n      <td>0.837349</td>\n      <td>0.004603</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.250000</td>\n      <td>0.045682</td>\n      <td>0.300898</td>\n      <td>0.106667</td>\n      <td>0.0</td>\n      <td>0.006748</td>\n      <td>0.000190</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.000000</td>\n      <td>(717.8, 734.4]</td>\n    </tr>\n    <tr>\n      <th>78629</th>\n      <td>0.284160</td>\n      <td>0.771160</td>\n      <td>1.0</td>\n      <td>0.000867</td>\n      <td>0.0</td>\n      <td>0.969880</td>\n      <td>0.003172</td>\n      <td>0.6</td>\n      <td>1.0</td>\n      <td>0.166667</td>\n      <td>0.015445</td>\n      <td>0.384731</td>\n      <td>0.080000</td>\n      <td>0.0</td>\n      <td>0.000346</td>\n      <td>0.000175</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>(734.4, 751.0]</td>\n    </tr>\n    <tr>\n      <th>78630</th>\n      <td>0.256251</td>\n      <td>0.573407</td>\n      <td>1.0</td>\n      <td>0.002638</td>\n      <td>1.0</td>\n      <td>0.903614</td>\n      <td>0.008350</td>\n      <td>0.5</td>\n      <td>1.0</td>\n      <td>0.583333</td>\n      <td>0.043223</td>\n      <td>0.215569</td>\n      <td>0.080000</td>\n      <td>0.0</td>\n      <td>0.004362</td>\n      <td>0.000140</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.333333</td>\n      <td>(734.4, 751.0]</td>\n    </tr>\n    <tr>\n      <th>78631</th>\n      <td>0.722088</td>\n      <td>0.760682</td>\n      <td>1.0</td>\n      <td>0.006569</td>\n      <td>0.0</td>\n      <td>0.475904</td>\n      <td>0.009817</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.250000</td>\n      <td>0.073832</td>\n      <td>0.173653</td>\n      <td>0.186667</td>\n      <td>0.0</td>\n      <td>0.016656</td>\n      <td>0.000454</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>(651.4, 668.0]</td>\n    </tr>\n    <tr>\n      <th>78632</th>\n      <td>0.883071</td>\n      <td>0.429344</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n      <td>0.987952</td>\n      <td>0.013377</td>\n      <td>0.6</td>\n      <td>1.0</td>\n      <td>0.250000</td>\n      <td>0.080573</td>\n      <td>0.176647</td>\n      <td>0.106667</td>\n      <td>0.0</td>\n      <td>0.009795</td>\n      <td>0.000373</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>(734.4, 751.0]</td>\n    </tr>\n    <tr>\n      <th>78633</th>\n      <td>0.092753</td>\n      <td>0.797050</td>\n      <td>1.0</td>\n      <td>0.003225</td>\n      <td>0.0</td>\n      <td>0.746988</td>\n      <td>0.006502</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.250000</td>\n      <td>0.037907</td>\n      <td>0.261976</td>\n      <td>0.106667</td>\n      <td>0.0</td>\n      <td>0.007822</td>\n      <td>0.000224</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.000000</td>\n      <td>(701.2, 717.8]</td>\n    </tr>\n    <tr>\n      <th>78634</th>\n      <td>0.714831</td>\n      <td>0.056370</td>\n      <td>1.0</td>\n      <td>0.000538</td>\n      <td>1.0</td>\n      <td>0.608434</td>\n      <td>0.005758</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.583333</td>\n      <td>0.043700</td>\n      <td>0.200599</td>\n      <td>0.146667</td>\n      <td>0.0</td>\n      <td>0.001566</td>\n      <td>0.000269</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.333333</td>\n      <td>(684.6, 701.2]</td>\n    </tr>\n    <tr>\n      <th>78635</th>\n      <td>0.333121</td>\n      <td>0.495984</td>\n      <td>1.0</td>\n      <td>0.001886</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>0.006117</td>\n      <td>0.8</td>\n      <td>0.0</td>\n      <td>0.250000</td>\n      <td>0.025192</td>\n      <td>0.350299</td>\n      <td>0.146667</td>\n      <td>0.0</td>\n      <td>0.008590</td>\n      <td>0.000492</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.666667</td>\n      <td>(734.4, 751.0]</td>\n    </tr>\n    <tr>\n      <th>78636</th>\n      <td>0.903626</td>\n      <td>0.842735</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>0.0</td>\n      <td>0.789157</td>\n      <td>0.009158</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>0.250000</td>\n      <td>0.030745</td>\n      <td>0.215569</td>\n      <td>0.186667</td>\n      <td>0.0</td>\n      <td>0.005010</td>\n      <td>0.000197</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.666667</td>\n      <td>(701.2, 717.8]</td>\n    </tr>\n    <tr>\n      <th>78637</th>\n      <td>0.882822</td>\n      <td>0.558630</td>\n      <td>1.0</td>\n      <td>0.002858</td>\n      <td>1.0</td>\n      <td>0.933735</td>\n      <td>0.005173</td>\n      <td>0.2</td>\n      <td>1.0</td>\n      <td>0.250000</td>\n      <td>0.042622</td>\n      <td>0.067365</td>\n      <td>0.093333</td>\n      <td>0.0</td>\n      <td>0.001116</td>\n      <td>0.000161</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>(734.4, 751.0]</td>\n    </tr>\n    <tr>\n      <th>78638</th>\n      <td>0.393072</td>\n      <td>0.748476</td>\n      <td>1.0</td>\n      <td>0.005064</td>\n      <td>0.0</td>\n      <td>0.801205</td>\n      <td>0.007600</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.250000</td>\n      <td>0.060716</td>\n      <td>0.419162</td>\n      <td>0.306667</td>\n      <td>0.0</td>\n      <td>0.024000</td>\n      <td>0.001820</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.000000</td>\n      <td>(717.8, 734.4]</td>\n    </tr>\n    <tr>\n      <th>78639</th>\n      <td>0.037834</td>\n      <td>0.302960</td>\n      <td>1.0</td>\n      <td>0.001646</td>\n      <td>1.0</td>\n      <td>0.939759</td>\n      <td>0.004814</td>\n      <td>0.9</td>\n      <td>1.0</td>\n      <td>0.250000</td>\n      <td>0.025881</td>\n      <td>0.258982</td>\n      <td>0.080000</td>\n      <td>0.0</td>\n      <td>0.010953</td>\n      <td>0.000300</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.666667</td>\n      <td>(734.4, 751.0]</td>\n    </tr>\n    <tr>\n      <th>78640</th>\n      <td>0.967447</td>\n      <td>0.354934</td>\n      <td>1.0</td>\n      <td>0.000997</td>\n      <td>1.0</td>\n      <td>0.909639</td>\n      <td>0.003359</td>\n      <td>0.2</td>\n      <td>1.0</td>\n      <td>0.250000</td>\n      <td>0.030108</td>\n      <td>0.155689</td>\n      <td>0.213333</td>\n      <td>0.0</td>\n      <td>0.006538</td>\n      <td>0.000211</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.666667</td>\n      <td>(734.4, 751.0]</td>\n    </tr>\n    <tr>\n      <th>78641</th>\n      <td>0.570822</td>\n      <td>0.119777</td>\n      <td>1.0</td>\n      <td>0.002489</td>\n      <td>0.0</td>\n      <td>0.156627</td>\n      <td>0.018769</td>\n      <td>0.4</td>\n      <td>0.0</td>\n      <td>0.250000</td>\n      <td>0.182550</td>\n      <td>0.254491</td>\n      <td>0.613333</td>\n      <td>0.0</td>\n      <td>0.042548</td>\n      <td>0.002385</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.666667</td>\n      <td>(601.6, 618.2]</td>\n    </tr>\n    <tr>\n      <th>78642</th>\n      <td>0.021688</td>\n      <td>0.047883</td>\n      <td>1.0</td>\n      <td>0.003415</td>\n      <td>0.0</td>\n      <td>0.608434</td>\n      <td>0.005059</td>\n      <td>0.3</td>\n      <td>1.0</td>\n      <td>0.250000</td>\n      <td>0.022015</td>\n      <td>0.079341</td>\n      <td>0.053333</td>\n      <td>0.0</td>\n      <td>0.009112</td>\n      <td>0.000225</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>(684.6, 701.2]</td>\n    </tr>\n    <tr>\n      <th>78643</th>\n      <td>0.302296</td>\n      <td>0.680785</td>\n      <td>1.0</td>\n      <td>0.004174</td>\n      <td>0.0</td>\n      <td>0.674699</td>\n      <td>0.012729</td>\n      <td>0.6</td>\n      <td>0.0</td>\n      <td>0.250000</td>\n      <td>0.038734</td>\n      <td>0.275449</td>\n      <td>0.120000</td>\n      <td>0.0</td>\n      <td>0.011715</td>\n      <td>0.000341</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>(684.6, 701.2]</td>\n    </tr>\n    <tr>\n      <th>78644</th>\n      <td>0.434638</td>\n      <td>0.529809</td>\n      <td>1.0</td>\n      <td>0.002096</td>\n      <td>1.0</td>\n      <td>0.915663</td>\n      <td>0.006995</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.250000</td>\n      <td>0.046720</td>\n      <td>0.086826</td>\n      <td>0.120000</td>\n      <td>0.0</td>\n      <td>0.008314</td>\n      <td>0.000266</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>(734.4, 751.0]</td>\n    </tr>\n    <tr>\n      <th>78645</th>\n      <td>0.635777</td>\n      <td>0.429482</td>\n      <td>1.0</td>\n      <td>0.004314</td>\n      <td>1.0</td>\n      <td>0.927711</td>\n      <td>0.008776</td>\n      <td>0.7</td>\n      <td>0.0</td>\n      <td>0.250000</td>\n      <td>0.044728</td>\n      <td>0.194611</td>\n      <td>0.133333</td>\n      <td>0.0</td>\n      <td>0.012751</td>\n      <td>0.000427</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.000000</td>\n      <td>(734.4, 751.0]</td>\n    </tr>\n    <tr>\n      <th>78646</th>\n      <td>0.769598</td>\n      <td>0.558257</td>\n      <td>1.0</td>\n      <td>0.001466</td>\n      <td>1.0</td>\n      <td>0.879518</td>\n      <td>0.008688</td>\n      <td>0.6</td>\n      <td>1.0</td>\n      <td>0.250000</td>\n      <td>0.011003</td>\n      <td>0.131737</td>\n      <td>0.106667</td>\n      <td>0.0</td>\n      <td>0.002648</td>\n      <td>0.000152</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>(717.8, 734.4]</td>\n    </tr>\n    <tr>\n      <th>78647</th>\n      <td>0.734902</td>\n      <td>0.423828</td>\n      <td>1.0</td>\n      <td>0.001213</td>\n      <td>1.0</td>\n      <td>0.801205</td>\n      <td>0.004149</td>\n      <td>0.4</td>\n      <td>0.0</td>\n      <td>0.250000</td>\n      <td>0.028455</td>\n      <td>0.092814</td>\n      <td>0.093333</td>\n      <td>0.0</td>\n      <td>0.002260</td>\n      <td>0.000214</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.333333</td>\n      <td>(717.8, 734.4]</td>\n    </tr>\n    <tr>\n      <th>78648</th>\n      <td>0.050081</td>\n      <td>0.972796</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n      <td>0.819277</td>\n      <td>0.005411</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.250000</td>\n      <td>0.028066</td>\n      <td>0.196108</td>\n      <td>0.093333</td>\n      <td>1.0</td>\n      <td>0.005626</td>\n      <td>0.000156</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.333333</td>\n      <td>(717.8, 734.4]</td>\n    </tr>\n    <tr>\n      <th>78649</th>\n      <td>0.082026</td>\n      <td>0.942814</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n      <td>0.981928</td>\n      <td>0.006063</td>\n      <td>0.6</td>\n      <td>0.0</td>\n      <td>0.250000</td>\n      <td>0.028083</td>\n      <td>0.239521</td>\n      <td>0.173333</td>\n      <td>0.0</td>\n      <td>0.005445</td>\n      <td>0.000395</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>(734.4, 751.0]</td>\n    </tr>\n  </tbody>\n</table>\n<p>78650 rows  20 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Preview data\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversample imbalanced dataset for classification"
   ]
  },
  {
   "source": [
    "Since we care about precision as much as we care about recall, we want precision and recall as balanced as possible and oversampling can be used to increase the minority data points.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df[[\"Credit Score Range\"]] = df[[\"Credit Score Range\"]].apply(le.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = [\"Credit Score\", \"Credit Score Range\"]\n",
    "X = df.drop(to_drop, axis = 1)\n",
    "labels = df[\"Credit Score Range\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "training data has 58987 observation with 18 features\ntest data has 19663 observation with 18 features\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.25, random_state=1)\n",
    "print('training data has ' + str(X_train.shape[0]) + ' observation with ' + str(X_train.shape[1]) + ' features')\n",
    "print('test data has ' + str(X_test.shape[0]) + ' observation with ' + str(X_test.shape[1]) + ' features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to_numpy()\n",
    "y_train = y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(0, 21423), (1, 21423), (2, 21423), (3, 21423), (4, 21423), (5, 21423), (6, 21423), (7, 21423), (8, 21423), (9, 21423)]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "from collections import Counter\n",
    "print(sorted(Counter(y_resampled).items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MSE for testing set:  0.020731413694460413 \n\nMSE for training set:  0.020935968132080082 \n\nR2 score for testing set:  0.2775364684968068 \n\nR2 score for training set:  0.2721337738409869 \n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = df.drop(columns = ['Loan ID', 'Customer ID', 'Credit Score Range', \n",
    "                       'Credit Score'])\n",
    "\n",
    "y = df['Credit Score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "LR_model = LinearRegression()\n",
    "LR_model.fit(X_train, y_train)\n",
    "y_pred = LR_model.predict(X_test)\n",
    "y_pred_train = LR_model.predict(X_train)\n",
    "## model evaluation\n",
    "print(\"MSE for testing set: \", mean_squared_error(y_test, y_pred), \"\\n\")\n",
    "print(\"MSE for training set: \", mean_squared_error(y_train, y_pred_train), \"\\n\")\n",
    "## The best R2 score is 1, it can be negative because the model is arbitrarily worse\n",
    "print(\"R2 score for testing set: \", r2_score(y_test, y_pred), \"\\n\")\n",
    "print(\"R2 score for training set: \", r2_score(y_train, y_pred_train), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for training set:  0.02072959867346114\n",
      "MSE for testing set:  0.02084024219386768 \n",
      "\n",
      "R2 score for training set:  0.2762557478125591\n",
      "R2 score for testing set:  0.2756088357369719\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(columns = ['Loan ID', 'Customer ID', 'Credit Score', 'Credit Score Range'])\n",
    "y = df['Credit Score']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "clf = svm.SVR(kernel='rbf')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('MSE for training set: ', mean_squared_error(y_train, y_pred_train))\n",
    "print('MSE for testing set: ', mean_squared_error(y_test, y_pred),'\\n')\n",
    "\n",
    "print('R2 score for training set: ', r2_score(y_train, y_pred_train))\n",
    "print('R2 score for testing set: ', r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression - Oversampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shiva\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for training set:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "(584.834, 601.6]       0.00      0.00      0.00       180\n",
      "  (601.6, 618.2]       0.00      0.00      0.00       321\n",
      "  (618.2, 634.8]       0.00      0.00      0.00       495\n",
      "  (634.8, 651.4]       0.00      0.00      0.00      1033\n",
      "  (651.4, 668.0]       0.10      0.00      0.00      2041\n",
      "  (668.0, 684.6]       0.15      0.00      0.00      3346\n",
      "  (684.6, 701.2]       0.16      0.03      0.05      6156\n",
      "  (701.2, 717.8]       0.26      0.05      0.09     11068\n",
      "  (717.8, 734.4]       0.27      0.35      0.30     15171\n",
      "  (734.4, 751.0]       0.45      0.84      0.59     18891\n",
      "\n",
      "        accuracy                           0.38     58702\n",
      "       macro avg       0.14      0.13      0.10     58702\n",
      "    weighted avg       0.29      0.38      0.29     58702\n",
      " \n",
      "\n",
      "Classification report for testing set:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "(584.834, 601.6]       0.00      0.00      0.00        69\n",
      "  (601.6, 618.2]       0.00      0.00      0.00       140\n",
      "  (618.2, 634.8]       0.00      0.00      0.00       243\n",
      "  (634.8, 651.4]       0.00      0.00      0.00       438\n",
      "  (651.4, 668.0]       0.44      0.01      0.02       855\n",
      "  (668.0, 684.6]       0.10      0.00      0.00      1417\n",
      "  (684.6, 701.2]       0.15      0.03      0.05      2640\n",
      "  (701.2, 717.8]       0.22      0.04      0.07      4576\n",
      "  (717.8, 734.4]       0.26      0.34      0.29      6560\n",
      "  (734.4, 751.0]       0.46      0.85      0.59      8221\n",
      "\n",
      "        accuracy                           0.38     25159\n",
      "       macro avg       0.16      0.13      0.10     25159\n",
      "    weighted avg       0.29      0.38      0.29     25159\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_resampled, y_resampled)\n",
    "\n",
    "y_pred_train = clf.predict(X_resampled)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Classification report for training set:\\n', classification_report(y_resampled, y_pred_train), '\\n')\n",
    "print('Classification report for testing set:\\n', classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "source": [
    "Assumption:Leverage bagging method to solve overfitting problems in complete decision trees\n",
    "\n",
    "Definition: Decision Trees that sample with replacement and randomly sample for features\n",
    "\n",
    "Objective of RandomForest: To introduce randomness and decrease variance into the model framework, thus prevent outliers impacting the model training process. \n",
    "\n",
    "Hyperparameter: number of decision trees, tree depth, number of tree nodes, number of splits of decision tree\n",
    "\n",
    "How Random Forest works in our dataset: \n",
    "For example, In our training data, we have 83150 observations with 18 features, random forest sample with replacement for 58987 rows of data from the original dataset so that each decision tree in the random forest has slightly different training data with the same distribution. Each decision tree has been selected with k features( k<18), for each sample of training data, a random forest will use k features to build the best decision tree. After repeating the tree building process, we get a group of different decision trees, thus we can combine them as a random forest model and perform regression for new data. \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the hyper-parameter tuning\n",
    "def class_plot(grid, grid_param, title):\n",
    "    scores = [x for x in grid.cv_results_['mean_test_score']]\n",
    "    m_depth = grid_param['max_depth']\n",
    "    n_est = grid_param['n_estimators']\n",
    "    #given a new shape of max_depth array into length of n estimators array without changing the data\n",
    "    scores = np.array(scores).reshape(len(m_depth), len(n_est))\n",
    "    fig = plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    for ind, i in enumerate(m_depth):\n",
    "        plt.plot(n_est, scores[ind], '-o', label='Max depth' + str(i),)\n",
    "    ax.legend(loc='lower right') #, bbox_to_anchor=(1, 0.5))\n",
    "    plt.xlabel('n estimator')\n",
    "    plt.ylabel('Mean score')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "training data has 58987 observation with 18 features\ntest data has 19663 observation with 18 features\n"
     ]
    }
   ],
   "source": [
    "to_drop = [\"Credit Score\", \"Credit Score Range\"]\n",
    "X = df.drop(to_drop, axis = 1)\n",
    "y = df[\"Credit Score\"]\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "print('training data has ' + str(X_train.shape[0]) + ' observation with ' + str(X_train.shape[1]) + ' features')\n",
    "print('test data has ' + str(X_test.shape[0]) + ' observation with ' + str(X_test.shape[1]) + ' features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    3.4s finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   5 out of   5 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   5 out of   5 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   5 out of   5 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    2.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    2.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    2.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  15 out of  15 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  15 out of  15 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  15 out of  15 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  15 out of  15 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  15 out of  15 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  15 out of  15 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   5 out of   5 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   5 out of   5 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   5 out of   5 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    1.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    2.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    2.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    2.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    2.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  15 out of  15 | elapsed:    3.7s finished\n",
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  15 out of  15 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  15 out of  15 | elapsed:    3.9s finished\n",
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  15 out of  15 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  15 out of  15 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  15 out of  15 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   5 out of   5 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   5 out of   5 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   5 out of   5 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    2.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    2.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    2.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    2.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    2.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  15 out of  15 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  15 out of  15 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  15 out of  15 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  15 out of  15 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  15 out of  15 | elapsed:    3.8s finished\n",
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  15 out of  15 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   5 out of   5 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   5 out of   5 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   5 out of   5 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    2.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    2.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    2.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  15 out of  15 | elapsed:    3.8s finished\n",
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  15 out of  15 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  15 out of  15 | elapsed:    3.7s finished\n",
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  15 out of  15 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  15 out of  15 | elapsed:    4.6s finished\n",
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  15 out of  15 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  15 out of  15 | elapsed:    5.1s finished\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=RandomForestRegressor(n_jobs=6, random_state=1,\n",
       "                                             verbose=1),\n",
       "             param_grid={'max_depth': [30, 40, 45, 50],\n",
       "                         'n_estimators': [2, 5, 10, 15]})"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_1 = {'n_estimators': [2, 5,10,15],\n",
    "          'max_depth': [30,40,45, 50]}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=1, verbose=1,n_jobs =6)\n",
    "#The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid.\n",
    "grid_rf = GridSearchCV(rf, grid_1, cv=3)\n",
    "grid_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "class_plot(grid_rf, grid_1, 'RF_CV_search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'max_depth': 45, 'n_estimators': 15}\n0.366235904023255\n"
     ]
    }
   ],
   "source": [
    "print (grid_rf.best_params_)\n",
    "print (grid_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  15 out of  15 | elapsed:    4.9s finished\n",
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  15 out of  15 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "# re-train the model with full training set\n",
    "rf_best = grid_rf.best_estimator_\n",
    "rf_best.fit(X_train, y_train)\n",
    "pred_rf_test = rf_best.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n[Parallel(n_jobs=6)]: Done  15 out of  15 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.38922568930438894"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "rf_best.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Feature importance ranking by Random Forest Model:\nTerm : 0.226\nMaximum Open Credit : 0.1161\nCurrent Credit Balance : 0.0914\nAnnual Income : 0.0741\nYears of Credit History : 0.0709\nCurrent Loan Amount : 0.0701\nMonthly Debt : 0.0691\nLoan ID : 0.0603\nCustomer ID : 0.0601\nNumber of Open Accounts : 0.0487\nPurpose : 0.0329\nYears in current job : 0.0274\nDelinquent Time : 0.0203\nHome Ownership : 0.0116\nLoan Status : 0.0089\nHave had Credit Problems : 0.0063\nHave had Bankruptcy before : 0.0044\nHave had Tax Liens : 0.0017\n"
     ]
    }
   ],
   "source": [
    "#show feature importance\n",
    "importances = rf_best.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature importance ranking by Random Forest Model:\")\n",
    "for ind in range(X.shape[1]):\n",
    "  print (\"{0} : {1}\".format(X.columns[indices[ind]],round(importances[indices[ind]], 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier(without oversample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = [\"Credit Score\", \"Credit Score Range\"]\n",
    "X = df.drop(to_drop, axis = 1)\n",
    "labels = df[\"Credit Score Range\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "training data has 58987 observation with 18 features\ntest data has 19663 observation with 18 features\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.25, random_state=1)\n",
    "print('training data has ' + str(X_train.shape[0]) + ' observation with ' + str(X_train.shape[1]) + ' features')\n",
    "print('test data has ' + str(X_test.shape[0]) + ' observation with ' + str(X_test.shape[1]) + ' features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.47101204 0.47516528 0.4714758  0.47715521 0.47495126]\nModel accuracy of Random Forest Classfier is 0.4739519169734986\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import model_selection\n",
    "classifier_RF = RandomForestClassifier()\n",
    "cv_score = model_selection.cross_val_score(classifier_RF, X_train, y_train, cv=5)\n",
    "print(cv_score)\n",
    "print('Model accuracy of Random Forest Classfier' + ' is ' + str(cv_score.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# helper function for printing out grid search results \n",
    "def print_grid_search_metrics(gs):\n",
    "    print (\"Best score: \" + str(gs.best_score_))\n",
    "    print (\"Best parameters set:\")\n",
    "    best_parameters = gs.best_params_\n",
    "    for param_name in sorted(best_parameters.keys()):\n",
    "        print(param_name + ':' + str(best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(),\n",
       "             param_grid={'n_estimators': [40, 60, 80]})"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "#find optimal hyperparameters of Random forest\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "    'n_estimators' : [40,60,80]\n",
    "}\n",
    "Grid_RF = GridSearchCV(RandomForestClassifier(),parameters, cv=5)\n",
    "Grid_RF.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best score: 0.4704426622695916\nBest parameters set:\nn_estimators:80\n"
     ]
    }
   ],
   "source": [
    "print_grid_search_metrics(Grid_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "best_RF_model = Grid_RF.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_pred_rf = best_RF_model.predict(X_train)\n",
    "ytest_pred_rf = best_RF_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.48069979148654834\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, ytest_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classification report for training set:\n               precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00       171\n           1       1.00      1.00      1.00       311\n           2       1.00      1.00      1.00       511\n           3       1.00      1.00      1.00      1011\n           4       1.00      1.00      1.00      1967\n           5       1.00      1.00      1.00      3088\n           6       1.00      1.00      1.00      5600\n           7       1.00      1.00      1.00      9751\n           8       1.00      1.00      1.00     15154\n           9       1.00      1.00      1.00     21423\n\n    accuracy                           1.00     58987\n   macro avg       1.00      1.00      1.00     58987\nweighted avg       1.00      1.00      1.00     58987\n \n\nClassification report for testing set:\n               precision    recall  f1-score   support\n\n           0       0.88      0.13      0.23        52\n           1       0.93      0.11      0.20       115\n           2       1.00      0.12      0.22       185\n           3       0.79      0.14      0.23       319\n           4       0.55      0.19      0.28       645\n           5       0.42      0.19      0.26      1069\n           6       0.39      0.23      0.29      1873\n           7       0.40      0.25      0.31      3327\n           8       0.37      0.41      0.39      5136\n           9       0.57      0.81      0.67      6942\n\n    accuracy                           0.48     19663\n   macro avg       0.63      0.26      0.31     19663\nweighted avg       0.47      0.48      0.45     19663\n \n\n"
     ]
    }
   ],
   "source": [
    "print('Classification report for training set:\\n', classification_report(y_train, ytrain_pred_rf), '\\n')\n",
    "print('Classification report for testing set:\\n', classification_report(y_test, ytest_pred_rf), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Feature importance ranking by Random Forest Model:\nMaximum Open Credit : 0.1068\nCurrent Credit Balance : 0.0986\nAnnual Income : 0.0931\nMonthly Debt : 0.092\nYears of Credit History : 0.0907\nLoan ID : 0.0887\nCustomer ID : 0.0886\nCurrent Loan Amount : 0.0858\nNumber of Open Accounts : 0.0673\nYears in current job : 0.0497\nTerm : 0.0448\nDelinquent Time : 0.0247\nPurpose : 0.0246\nHome Ownership : 0.0197\nLoan Status : 0.0103\nHave had Credit Problems : 0.007\nHave had Bankruptcy before : 0.0053\nHave had Tax Liens : 0.0024\n"
     ]
    }
   ],
   "source": [
    "#show feature importance\n",
    "importances = best_RF_model.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature importance ranking by Random Forest Model:\")\n",
    "for ind in range(X.shape[1]):\n",
    "  print (\"{0} : {1}\".format(X.columns[indices[ind]],round(importances[indices[ind]], 4)))"
   ]
  },
  {
   "source": [
    "5-fold cross validation was applied to Random Forest with dependent variables being the labeled encoded target score range. The accuracy of Random Forest classifier for training data has improved to 0.47. Then grid search cross validation is used to select the best hyperparameters among the number of decision trees as 40, 60, and 80. The number of decision trees was 80 and the accuracy after grid search remained to be 0.47. The accuracy of the test data turned out to be 0.48.\n",
    "The precision and recall for the training set showed an overfitting trend, it might be due to the imbalance in dataset characteristics.The precision was much higher than the recall for prediction of lower credit score ranges (0 ~7), but the weighted average recall was 0.48 and the weighted average precision was 0.47.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier( with oversample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.91016664 0.90829949 0.92230313 0.94767306 0.96968212]\nModel accuracy of Random Forest Classfier is 0.9316248891378425\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import model_selection\n",
    "classifier_RF = RandomForestClassifier()\n",
    "cv_score = model_selection.cross_val_score(classifier_RF, X_resampled, y_resampled, cv=5)\n",
    "print(cv_score)\n",
    "print('Model accuracy of Random Forest Classfier' + ' is ' + str(cv_score.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# helper function for printing out grid search results \n",
    "def print_grid_search_metrics(gs):\n",
    "    print (\"Best score: \" + str(gs.best_score_))\n",
    "    print (\"Best parameters set:\")\n",
    "    best_parameters = gs.best_params_\n",
    "    for param_name in sorted(best_parameters.keys()):\n",
    "        print(param_name + ':' + str(best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(),\n",
       "             param_grid={'n_estimators': [40, 60, 80]})"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "#find optimal hyperparameters of Random forest through grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "    'n_estimators' : [40,60,80]\n",
    "}\n",
    "Grid_RF = GridSearchCV(RandomForestClassifier(),parameters, cv=5)\n",
    "Grid_RF.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best score: 0.9300611492321338\nBest parameters set:\nn_estimators:80\n"
     ]
    }
   ],
   "source": [
    "print_grid_search_metrics(Grid_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "best_RF_model = Grid_RF.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_pred_rf = best_RF_model.predict(X_resampled)\n",
    "ytest_pred_rf = best_RF_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.4861923409449219\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, ytest_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classification report for training set:\n               precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     21423\n           1       1.00      1.00      1.00     21423\n           2       1.00      1.00      1.00     21423\n           3       1.00      1.00      1.00     21423\n           4       1.00      1.00      1.00     21423\n           5       1.00      1.00      1.00     21423\n           6       1.00      1.00      1.00     21423\n           7       1.00      1.00      1.00     21423\n           8       1.00      1.00      1.00     21423\n           9       1.00      1.00      1.00     21423\n\n    accuracy                           1.00    214230\n   macro avg       1.00      1.00      1.00    214230\nweighted avg       1.00      1.00      1.00    214230\n \n\nClassification report for testing set:\n               precision    recall  f1-score   support\n\n           0       0.69      0.17      0.28        52\n           1       0.72      0.16      0.26       115\n           2       0.77      0.18      0.30       185\n           3       0.71      0.20      0.32       319\n           4       0.48      0.25      0.33       645\n           5       0.38      0.27      0.32      1069\n           6       0.38      0.32      0.35      1873\n           7       0.39      0.30      0.34      3327\n           8       0.40      0.40      0.40      5136\n           9       0.59      0.77      0.67      6942\n\n    accuracy                           0.49     19663\n   macro avg       0.55      0.30      0.35     19663\nweighted avg       0.47      0.49      0.47     19663\n \n\n"
     ]
    }
   ],
   "source": [
    "print('Classification report for training set:\\n', classification_report(y_resampled, ytrain_pred_rf), '\\n')\n",
    "print('Classification report for testing set:\\n', classification_report(y_test, ytest_pred_rf), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Feature importance ranking by Random Forest Model:\nMaximum Open Credit : 0.0971\nAnnual Income : 0.0944\nMonthly Debt : 0.0933\nCurrent Credit Balance : 0.0926\nYears of Credit History : 0.0918\nLoan ID : 0.0914\nCustomer ID : 0.0913\nCurrent Loan Amount : 0.0853\nNumber of Open Accounts : 0.0719\nYears in current job : 0.0486\nTerm : 0.0367\nDelinquent Time : 0.0288\nPurpose : 0.0261\nHome Ownership : 0.0209\nLoan Status : 0.0109\nHave had Credit Problems : 0.0085\nHave had Bankruptcy before : 0.0072\nHave had Tax Liens : 0.003\n"
     ]
    }
   ],
   "source": [
    "#show feature importance\n",
    "importances = best_RF_model.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature importance ranking by Random Forest Model:\")\n",
    "for ind in range(X.shape[1]):\n",
    "  print (\"{0} : {1}\".format(X.columns[indices[ind]],round(importances[indices[ind]], 4)))"
   ]
  },
  {
   "source": [
    "5 fold cross validation was applied to select the best hyperparameter for the Random Forest model trained with a resampled dataset. The accuracy after oversampling turned out to be 0.93. Grid search with 5 fold cross validation was also applied to random forest classifiers, the accuracy after grid search turned out to be 0.93  . The best hyperparameter for the number of decision trees was 80. The accuracy for the test data was 0.50. Even though the precision and recall still showed an overfitting trend for training data, the test data has improved weighted average recall to be 0.49, and the overall recall scores for all credit score ranges showed more balanced precision. \n",
    "The most contributing factors for Random Forest classifier after oversampling was maximum open credit, annual income and monthly debt. \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}